\documentclass{article}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{epigraph} 

\title{Linear Algebra Done Right\\Solutions to Exercises 5.C}
\author{}
\date{}

\begin{document}

\maketitle

\section{Diagonalizability implies $V=\operatorname{null}T\oplus\operatorname{range}T$}
\subsection*{Problem statement}
Suppose $T\in\mathcal{L}(V)$ is diagonalizable. 
Prove that $V=\operatorname{null}T\oplus\operatorname{range}T$.

\subsection*{Solution}
Suppose $v\in\operatorname{range}T$. 
This implies there exists $u\in V$ such that $Tu=v$. 
Via Theorem 5.41 (`Conditions equivalent to diagonalizability'), it follows that $V$ has a basis consisting of eigenvectors of $T$. 
Suppose $v_1,\ldots,v_n$ is this basis and we can write $u$ as
\[u=a_1v_1 +\cdots +a_nv_n.\]
Applying $T$ to both sides, we have
\[v=a_1Tv_1 +\cdots +a_nTv_n=a_1\lambda_1v_1 +\cdots + a_n\lambda_n v_n.\]
However, some eigenvalues may equal $0$.
Suppose the first $k$ eigenvectors correspond to an eigenvalue of $0$. 
Thus, we can write
\[v=a_{k+1}\lambda_{k+1}v_{k+1}+\cdots+a_n\lambda_n v_n\]
showing that $\operatorname{range}T$ is a subset of the direct sum of all eigenspaces corresponding to nonzero eigenvalues. 
It clearly follows that the direct sum of all eigenspaces corresponding to nonzero eigenvalues is a subset of $\operatorname{range}T$, implying that $\operatorname{range}T$ is equal to the direct sum of all eigenspaces corresponding to nonzero eigenvalues.
Since $\operatorname{null}T=E(0,T)$ and Theorem 5.41 implies 
\[V=E(\lambda_1,T)\oplus\cdots\oplus E(\lambda_m,T),\]
it immediately follows that $V=\operatorname{null}T\oplus\operatorname{range}T$\footnote{I'm not entirely satisfied with the structure of this proof.}.

\clearpage

\section{$V=\operatorname{null}T\oplus\operatorname{range}T$ does not imply diagonal $T$}
\subsection*{Problem statement}
Prove the converse of the statement in the exercise above or give a counterexample to the converse.

\subsection*{Solution}
Like all counterexamples in Chapter 5, rotations are our friend.
Suppose $T\in\mathcal{L}(\mathbf{R}^2)$ is defined by
\[T(w,z)=(-z,w).\]
Clearly $\operatorname{null}T=\{0\}$. 
Therefore, via the Fundamental Theorem of Linear Maps (Theorem 3.22), $\operatorname{dim}\operatorname{range}T=2$ and $\mathbf{R}^2=\operatorname{range}T$. 
Thus, we have 
\[\mathbf{R}^2=\operatorname{null}T\oplus\operatorname{range}T.\]
However, it was shown in Example 5.8 that $T$ has no eigenvalues. 
Hence $T$ is not diagonalizable and the converse of Exercise 5.C(1) does not hold.

\clearpage

\section{Conditions equivalent to $V=\operatorname{null}T\oplus\operatorname{range}T$}
\subsection*{Problem statement}
Suppose $V$ is finite-dimensional and $T\in\mathcal{L}(V)$. 
Prove that the following are equivalent:
\begin{itemize}
    \item[(a)] $V=\operatorname{null}T\oplus\operatorname{range}T$
    \item[(b)] $V=\operatorname{null}T+\operatorname{range}T$
    \item[(c)] $\operatorname{null}T\cap \operatorname{range}T=\{0\}$
\end{itemize}

\subsection*{Solution}
Clearly (a) implies (b). 

Suppose (b). 
Via Theorem 2.43 (`Dimension of a sum'), we can write
\[\dim(\operatorname{null}T+\operatorname{range}T)=\dim\operatorname{null}T+\dim\operatorname{range}T-\dim(\operatorname{null}T\cap \operatorname{range}T),\]
and via the Fundamental Theorem of Linear Maps (Theorem 3.22), we can also write
\[\dim V=\dim\operatorname{null}T+\dim\operatorname{range}T.\]
Combining our two expressions, it follows that $\dim(\operatorname{null}T\cap \operatorname{range}T)=0$ and $\operatorname{null}T\cap \operatorname{range}T=\{0\}$. Hence (b) implies (c).

Suppose (c). 
Theorem 1.45 (`Direct sum of two subspaces') tells us that $\operatorname{null}T\oplus\operatorname{range}T$. 
To show that this direct sum equals $V$, via Theorem 2.43, we can write
\[\dim(\operatorname{null}T+\operatorname{range}T)=\dim\operatorname{null}T+\dim\operatorname{range}T-\dim(\operatorname{null}T\cap \operatorname{range}T),\]
and by using the Fundamental Theorem of Linear Maps, it follows that
\[\dim(\operatorname{null}T+\operatorname{range}T)=\dim(V)-\dim(\operatorname{null}T\cap \operatorname{range}T).\]
Hence, we have $\dim(V)=\dim(\operatorname{null}T+\operatorname{range}T)$ and it follows that 
\[V=\operatorname{null}T\oplus\operatorname{range}T.\]
Therefore, (c) implies (a).

\clearpage

\section{Exercise 5.C(3) with infinite dimensions}
\subsection*{Problem statement}
Given an example to show that the exercise above is false with the hypothesis that $V$ is finite-dimensional.

\subsection*{Solution}
Infinite-dimensional counterexamples are usually the shift operators. 
This exercise is no exception.

Define $T\in\mathcal{L}(\mathbf{F}^\infty)$ by
\[T(x_1,x_2,x_3,\ldots)=(x_2,x_3,\ldots).\]
Clearly $\operatorname{range}T=\mathbf{F}^\infty$ but $(1,0,0,\ldots)\in\operatorname{null}T$. 
Hence we have
\[\mathbf{F}^\infty=\operatorname{null}T+\operatorname{range}T,\]
but $\operatorname{null}T\cap \operatorname{range}T\neq \{0\}.$

\clearpage

\section{Conditions for diagonalizability in $\mathbf{C}$}
\subsection*{Problem statement}
Suppose $V$ is a finite-dimensional complex vector space and $T\in\mathcal{L}(V)$. 
Prove that $T$ is diagonalizable if and only if 
\[V=\operatorname{null}(T-\lambda I)\oplus\operatorname{range}(T-\lambda I)\]
for every $\lambda\in\mathbf{C}$.

\subsection*{Solution}
\subsubsection*{First Direction}
Suppose $T$ is diagonalizable. 
Suppose the diagonal matrix of $T$ is given by
\[\mathcal{M}(T)=\begin{pmatrix}\lambda_1 & & 0\\ & \ddots & \\0 & & \lambda_n\end{pmatrix}.\]
Let $\lambda\in\mathbf{C}$. Then
\[\mathcal{M}(T-\lambda I)=\begin{pmatrix}\lambda_1-\lambda & & 0\\ & \ddots & \\0 & & \lambda_n-\lambda\end{pmatrix}.\]
implying that $T-\lambda I$ is diagonalizable regardless of whether $\lambda$ is an eigenvalue of $T$.
Hence, via our result in Exercise 5.C(1), it follows that 
\[V=\operatorname{null}(T-\lambda I)\oplus\operatorname{range}(T-\lambda I).\] 

\subsubsection*{Second Direction}
Notice that if $\dim V=1$, than this direction is trivially true. 
More specifically, via Theorem 5.21 (`Operators on complex vector space have an eigenvalue'), $T$ has an eigenvalue $\lambda_0$, and by setting $\lambda=\lambda_0$, we can write
\[V=\operatorname{null}(T-\lambda_0 I)\oplus\operatorname{range}(T-\lambda_0I)=E(\lambda_0,T)\oplus \operatorname{range}(T-\lambda_0I).\]
Since $E(\lambda_0,T)$ is a subspace of $V$ and $\operatorname{dim}E(\lambda_0,T)\geq 1$, then $\operatorname{dim}E(\lambda_0,T)=1$ and $\dim \operatorname{range}(T-\lambda_0I)=0$. 
Therefore, $\operatorname{range}(T-\lambda_0I)=\{0\}$ and we have
\[V=E(\lambda_0,T),\]
implying that $T$ is diagonalizable via Theorem 5.41 (`Conditions equibvalent to diagonalizability').

Hence, we can prove this direction via induction. 

Suppose that 
\[V=\operatorname{null}(T-\lambda I)\oplus\operatorname{range}(T-\lambda I)\]
for every $\lambda\in\mathbf{C}$ and the desired result holds for all complex vector spaces whose dimension is $\dim V-1$ or less. 
Via Theorem 5.41, $T$ has an eigenvalue $\lambda_0$ and we can write
\[V=E(\lambda_0,T)\oplus \operatorname{range}(T-\lambda_0 I).\]
Notice that $\operatorname{range}(T-\lambda_0 I)$ is invariant under $T$. 
To see this, let\newline $w\in \operatorname{range}(T-\lambda_0 I)$. 
Then by definition there exists $v\in V$ such that
\[w=(T-\lambda_0 I)(v).\]
Applying T to both sides, we have
\[T(w)=T((T-\lambda_0 I)(v))=(T-\lambda_0 I)(T(v)),\]
showing that $T(w)\in \operatorname{range}(T-\lambda_0 I)$. 
Now here comes the fun part. 

It clearly follows that $\dim \operatorname{range}(T-\lambda_0 I)\leq \dim V$. 
Hence, via our induction hypothesis, $T|_{\operatorname{range}(T-\lambda_0 I)}$ is diagonalizable and there exist a basis of\newline $\operatorname{range}(T-\lambda_0 I)$ consisting of eigenvectors of $T$. 
Appending the eigenvectors of $\lambda_0$ to the basis of $\operatorname{range}(T-\lambda_0 I)$, we have a basis of $V$ consisting of eigenvectors of $T$. 
Therefore, via Theorem 5.41, $T$ is diagonalizable.

\clearpage

\section{If $S,T$ share eigenvectors, then $ST=TS$}
\subsection*{Problem statement}
Suppose $V$ is finite-dimensional, $T\in\mathcal{L}(V)$ has $\dim V$ distinct eigenvalues, and $S\in\mathcal{L}(V)$ has the same eigenvectors as $T$ (not necessarily with same eigenvalues).
Prove that $ST=TS$.

\subsection*{Solution}
Via Theorem 5.44 (`Enough eigenvalues implies diagonalizability'), we know that $T$ is diagonalizable. 
Given Theorem 5.41 (`Conditions equivalent to diagonalizability'), it follows that $V$ has a basis consisting of eigenvectors of $T$. 
Since $S$ has the same eigenvectors as $T$, it follows that $S$ is also diagonalizable. 
Hence, $S$ and $T$ have diagonalizable matrices with respect to the same basis.

For the next part of the proof, we are making use of the fact that the product of two diagonal matrices commutes, an observation that is easily verified. 
Thus, we can write
\[\mathcal{M}(S)\mathcal{M}(T)=\mathcal{M}(T)\mathcal{M}(S),\]
and following from Theorem 3.43 (`The matrix of the product of linear maps'), we have
\[\mathcal{M}(ST)=\mathcal{M}(TS),\]
which, given $\mathcal{M}$ is an isomorphism (Theorem 3.60), implies that
\[ST=TS.\]

\clearpage

\section{$\lambda$'s appearance on the diagonal of $A$}
\subsection*{Problem statement}
Suppose $T\in\mathcal{L}(V)$ has a diagonal matrix $A$ with respect to some basis of $V$ and that $\lambda\in\mathbf{F}$.
Prove that $\lambda$ appears on the diagonal of $A$ precisely $\dim E(\lambda,T)$ times.

\subsection*{Solution}
If $\lambda$ is not an eigenvalue, then $\dim E(\lambda,T)=0$. 
Via Theorem 5.32 (`Determination of eigenvalues from upper-triangular matrix'), only the eigenvalues of $T$ appear on the diagonal of $A$. 
Hence, $\lambda$ will appear on the diagonal of $A$ precisely $0=\dim E(\lambda,T)$ times.

If $\lambda$ is an eigenvalue, then $\dim E(\lambda,T)>0$. 
Suppose $v_1,\ldots,v_n$ is the basis of $V$ with which $A$ is a diagonal matrix. 
It follows that each vector in $v_1,\ldots,v_n$ is an eigenvector of $T$. 
Precisely $\dim E(\lambda,T)$ of these vectors correspond to the eigenvalue $\lambda$. 
Hence, $\lambda$ will appear on the diagonal of $A$ precisely $\dim E(\lambda,T)$ times.

\clearpage

\section{Eigenvalues when $\dim E(8,T)=4$}
\subsection*{Problem statement}
Suppose $T\in\mathcal{L}(\mathbf{F}^5)$ and $\dim E(8,T)=4$. 
Prove that $T-2I$ or $T-6I$ is invertible.

\subsection*{Solution}
Via Theorem 5.6 (`Equivalent conditions to be an eigenvalue'), $T-2I$ or $T-6I$ being invertible correspond to either $2$ or $6$ not being eigenvalues. 

Suppose $2$ and $6$ are both eigenvalues of $T$. 
It follows that 
\[\dim E(2,T)\geq 1\qquad\text{and}\qquad\dim E(6,T)\geq 1.\]
Given Theorem 5.38 (`Sum of eigenspaces is a direct sum'), it follows that
\begin{align*}
    5=\dim \mathbf{F}^5 &\geq \dim E(8,T)+\dim E(2,T)+\dim E(6,T)\\
    &=4+\dim E(2,T)+\dim E(6,T)
\end{align*}
and by rearranging terms, we have
\[1\geq\dim E(2,T) + \dim E(6,T).\]
Hence, we have a contradiction and $2$ and $6$ cannot both be eigenvalues of $T$. 
Therefore, it must be the case that $T-2I$ or $T-6I$ is invertible.

\clearpage

\renewcommand{\thesection}{10}
\section{Eigenspaces with nonzero eigenvalues}
\subsection*{Problem statement}
Suppose that $V$ is finite-dimensional and $T\in\mathcal{L}(V)$. 
Let $\lambda_1,\ldots,\lambda_m$ denote the distinct nonzero eigenvalues of $T$. 
Prove that
\[\dim E(\lambda_1,T)+\cdots+\dim E(\lambda_m,T)\leq \dim\operatorname{range}T.\]

\subsection*{Solution}
Via Example 5.3, we know that $\operatorname{range}T$ is invariant under $T$. 
Hence, let's examine the operator $T|_{\operatorname{range}T}$. If we can show that $E(\lambda_j,T|_{\operatorname{range}T})$ is well-defined and
\[E(\lambda_j,T)=E(\lambda,T|_{\operatorname{range}T})\]
for $j=1,\ldots,m$, then the desired result follows as a consequence of Theorem 5.38 (`Sum of eigenspaces is a direct sum').

Suppose $v\in E(\lambda_j,T)$ and $Tv=\lambda_j v$. 
Hence, $\lambda_jv\in \operatorname{range}T$. 
By noting that\footnote{This is possible since $\lambda_j$ is a nonzero eigenvalue.}
\[T(\frac{1}{\lambda_j}v)=v,\]
it follows that $v\in\operatorname{range}T$ and
\[E(\lambda_j,T)\subset \operatorname{range}T.\]
Therefore, $E(\lambda_j,T|_{\operatorname{range}T})$ is well-defined.

To show $E(\lambda_j,T)=E(\lambda,T|_{\operatorname{range}T})$, first note that our reasoning in the previous paragraph implies
\[E(\lambda_j,T)\subset E(\lambda,T|_{\operatorname{range}T}).\]
To show the other direction, suppose $v\in E(\lambda,T|_{\operatorname{range}T})$. 
We can write
\[T|_{\operatorname{range}T}(v)=Tv=\lambda_j v.\]
Hence, it follows that $v\in E(\lambda_j,T)$.

At this point, we've shown $E(\lambda_j,T|_{\operatorname{range}T})$ is well-defined and\newline $E(\lambda,T|_{\operatorname{range}T})=E(\lambda_j,T)$. 
Via Theorem 5.38, we can write
\[E(\lambda_1,T|_{\operatorname{range}T})+\cdots+E(\lambda_m,T|_{\operatorname{range}T})\leq \dim\operatorname{range}T,\]
and it immediately follows that
\[\dim E(\lambda_1,T)+\cdots+\dim E(\lambda_m,T)\leq \dim\operatorname{range}T.\]

\clearpage

\renewcommand{\thesection}{12}
\section{$R=S^{-1}TS$ if $R,T$ share eigenvalues}
\subsection*{Problem statement}
Suppose $R,T\in\mathcal{L}(\mathbf{F}^3)$ each have $2,6,7$ as eigenvalues. 
Prove that there exists an invertible operator $S\in\mathcal{L}(\mathbf{F}^3)$ such that $R=S^{-1}TS$.

\subsection*{Solution}
The idea is to use Theorem 3.5 (`Linear maps and basis of domain') to convert eigenvectors of $R$ to eigenvectors of $T$.

Following from Theorem 5.44 (`Enough eigenvalues implies diagonalizability'), we know that $R$ and $T$ are diagonalizable. Thus, via Theorem 5.41 (`Conditions equivalent to diagonalizability'), $V$ has a basis consisting of eigenvectors of $R$ and a basis consisting of eigenvectors of $T$. 
Suppose $v_1,v_2,v_3$ are a basis of $V$ consisting of eigenvectors of $R$ such that $Rv_1=2v_1$, $Rv_2=6v_2$, and $Rv_3=7v_3$. 
Suppose $u_1,u_2,u_3$ are a basis of $V$ consisting of eigenvectors of $T$ such that $Tv_1=2v_1$, $Tv_2=6v_2$, and $Tv_3=7v_3$. 

Via Theorem 3.5, there exists an operator $S\in\mathcal{L}(\mathbf{F}^3)$ such that $Sv_j=u_j$ for $j=1,2,3$. 
This operator $S$ is clearly invertible with $S^{-1}u_j=v_j$.
Now we are ready for the magic. 
Since $v_1,v_2,v_3$ is a basis of $\mathbf{F}^3$, we can write any vector $v\in\mathbf{F}^3$ as a linear combination
\[v=a_1v_1+a_2v_2+a_3v_3\]
for some constants $a_1,a_2,a_3\in\mathbf{F}$. 
Hence, we can write
\begin{align*}
    S^{-1}TS(v)&=S^{-1}TS(a_1v_1+a_2v_2+a_3v_3)\\
    &=S^{-1}T(a_1u_1+a_2u_2+a_3u_3)\\
    &=S^{-1}(a_12u_1+a_26u_2+a_37u_3)\\
    &=a_12v_1+a_26v_2+a_37v_3\\
    &=a_1Rv_1+a_2Rv_2+a_3Rv_3\\
    &=R(a_1v_1+a_2v_2+a_3v_3)\\
    &=Rv.
\end{align*}
Therefore, if follows that $R=S^{-1}TS$.

\clearpage

\renewcommand{\thesection}{16}
\section{Computing the Fibonacci sequence}
\subsection*{Problem statement}
The \textbf{\textit{Fibonacci sequence}} $F_1,F_2,\ldots$ is defined by
\[F_1=1,\;\;F_2=1,\quad\text{and}\quad F_n=F_{n-2}+F_{n-1}\text{ for }n\geq3.\]
Define $T\in\mathcal{L}(\mathbf{R}^2)$ by $T(x,y)=(y,x+y)$.

\begin{itemize}
    \item[(a)] Show that $T^n(0,1)=(F_n,F_{n+1})$ for each positive integer $n$.
    \item[(b)] Find the eigenvalues of $T$.
    \item[(c)] Find a basis of $\mathbf{R}^2$ consisting of eigenvectors of $T$.
    \item[(d)] Use the solution to part (c) to compute $T^n(0,1)$. Conclude that \[F_n=\frac{1}{\sqrt{5}}\Big[\Big(\frac{1+\sqrt{5}}{2}\Big)^n-\Big(\frac{1-\sqrt{5}}{2}\Big)^n\Big]\] for each positive integer $n$.
    \item[(e)] Use part (d) to conclude that for each positive integer $n$, the Fibonacci number $F_n$ is the integer that is closest to \[\frac{1}{\sqrt{5}}\Big(\frac{1+\sqrt{5}}{2}\Big)^n.\]
\end{itemize}

\subsection*{Solution}
This is an outstanding problem and I recommend that you solve it on your own. I wrote my solution up in my blog: \url{https://keith-murray.github.io/Posts/fibonacci}.


\end{document}