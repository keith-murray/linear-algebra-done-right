\documentclass{article}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{epigraph}
\usepackage{csquotes}
\usepackage{mathtools}

\title{Linear Algebra Done Right\\Solutions to Exercises 8.B}
\author{}
\date{}

\begin{document}

\maketitle

\section{If $0$ is only eigenvalue, $N$ is nilpotent}
\subsection*{Problem statement}
Suppose $V$ is a complex vector space, $N\in\mathcal{L}(V)$, and $0$ is the only eigenvalue of $N$. 
Prove that $N$ is nilpotent.

\subsection*{Solution}
Following Theorem 5.27 (`Over $\mathbf{C}$, every operator has an upper-triangular matrix') and Theorem 5.32 (`Determination of eigenvalues from upper-triangular matrix'), we can infer that $N$ has an upper-triangular matrix $\mathcal{M}(N)$ such that the only entries on the diagonal are zeros. 
Now we must prove that such a matrix is nilpotent.

Let $v_1,\ldots,v_n$ be the basis associated with $\mathcal{M}(N)$. 
Clearly $N^1v_1=0$. 
To prove that $N^j(v_j)=0$ for $j\in\{2,\ldots,n\}$, let's use induction and suppose $N^k(v_k)=0$ for $k<j$. 
Since $\mathcal{M}(N)$ is upper-triangular and diagonal entries are $0$'s, it follows that 
\[N^j(v_j)=N^{j-1}(a_1v_1+\cdots+a_{j-1}v_{j-1})=a_1N^{j-2}(N^1v_1)+\cdots+a_{j-1}N^{j-1}v_{j-1}.\]
Via our induction hypothesis, it follows that $N^j(v_j)=0$.

To show that $N^{\operatorname{dim}V}=0$, suppose $v\in V$. 
Given $v_1,\ldots,v_n$ is a basis of $V$, we can write
\[v=a_1v_1+\cdots+a_nv_n\]
and applying $N^n$ to both sides (where $N^n=N^{\operatorname{dim}V}$), we have
\[N^n(v)=a_1N^n(v_1)+\cdots+a_nN^n(v_n)=a_1N^{n-1}(N^1v_1)+\cdots+a_nN^n(v_n)=0\]
via our reasoning above. 
Hence, $N^{\operatorname{dim}V}=0$ and $N$ is nilpotent.

\subsection*{Notes}
This exercise and Exercise 8.A(7) imply that for a complex vector space $V$, $N\in\mathcal{L}(V)$ is nilpotent if and only if $0$ is the only eigenvalue of $N$.

\clearpage

\section{Eigenvalue $0$ doesn't entail nilpotent over $\mathbf{R}$}
\subsection*{Problem statement}
Give an example of an operator $T$ on a finite-dimensional real vector pace such tht $0$ is the only eigenvalue of $T$ but $T$ is not nilpotent.

\subsection*{Solution}
Suppose $T\in\mathcal{L}(\mathbf{R}^3)$ is defined by
\[T(z_1,z_2,z_3)=(-z_2,z_1,0).\]
Clearly $0$ is an eigenvalue since $T(0,0,1)=(0,0,0)$; however, there are no other eigenvalues since the operator is otherwise a rotation in the first and second coordinates. 
More specifically, for the subspace 
\[U=\{(z_1,z_2,0)\in\mathbf{R}^3:z_1,z_2\in\mathbf{R}\},\] 
$T$ is a counterclockwise rotation in $U$.

To show that $T$ is not nilpotent, we need only compute $T^3$ via Theorem 8.18 (`Nilpotent operator raised to dimension of domain is $0$'). 
Hence, we write
\begin{align*}
    T^3(z_1,z_2,z_3)&=T^2(-z_2,z_1,0)\\
    &=T(-z_1,-z_2,0)\\
    &=(z_2,-z_1,0).
\end{align*}

\subsection*{Notes}
We previously noted that Exercise 8.B(1) and Exercise 8.A(7) imply that for a complex vector space $V$, $N\in\mathcal{L}(V)$ is nilpotent if and only if $0$ is the only eigenvalue of $N$. 
This exercise shows us that for a real vector space $V$, we can only state that if $N\in\mathcal{L}(V)$ is nilpotent, then $0$ is the only eigenvalue of $N$. 
This exercise is a counterexample against the other direction.

This result is another implication of the failure of the Fundamental Theorem of Algebra for $\mathbf{R}$.

\clearpage

\section{Multiplicity of eigenvalues for $T$ and $S^{-1}TS$}
\subsection*{Problem statement}
Suppose $T\in\mathcal{L}(V)$. 
Suppose $S\in\mathcal{L}(V)$ is invertible. 
Prove that $T$ and $S^{-1}TS$ have the same eigenvalues with the same multiplicities.

\subsection*{Solution}
This exercise bears a strong resemblance to Exercise 5.A(15) whereby we proved that $T$ and $S^{-1}TS$ have the same eigenvalues and that $v\in V$ is an eigenvector of $S^{-1}TS$ if and only if $Sv$ is an eigenvector of $T$. 
To show that $T$ and $S^{-1}TS$ have the same multiplicities, we must show 
\[\operatorname{dim}G(\lambda,T)=\operatorname{dim}G(\lambda,S^{-1}TS)\]
for all eigenvalues $\lambda$.

Suppose $v\in V$ is a generalized eigenvector of $T$ corresponding to the eigenvalue $\lambda$. 
Hence, we have $(T-\lambda I)^jv=0$ for some positive integer $j$. 
Given $S$ is invertible, it follows that $S^{j}$ is invertible\footnote{
If one must be convinced of this, note that $\operatorname{range}S$ and $\operatorname{range}S^j$ are equivalent. Since $S$ is surjective, $S^j$ must be as well and is thus invertible.
}. 
If follows that $v\in\operatorname{range}S^j$ and there exists $u\in V$ such that $S^ju=v$ and $S^{-j}v=u$. 
Now we can write
\begin{align*}
    0=S^{-j}(0)&=S^{-j}(T-\lambda I)^jv\\
    &=S^{-j}(T-\lambda I)^jS^ju\\
    &=(S^{-1}TS-\lambda S^{-1}IS)u\\
    &=(S^{-1}TS-\lambda I)u.
\end{align*}
Hence $u=S^{-j}v$ is a generalized eigenvector of $S^{-1}TS$.

Suppose $v\in V$ is a generalized eigenvector of $S^{-1}TS$ corresponding to the eigenvalue $\lambda$. 
Hence, we have $(S^{-1}TS-\lambda I)^jv=0$ for some positive integer $j$. 
Using similar logic as before, there exists $u\in V$ such that $S^{-j}u=v$. 
Now we can write
\begin{align*}
    0=S^{j}(0)&=S^{j}(S^{-1}TS-\lambda I)^jv\\
    &=S^{j}(S^{-1}TS-\lambda I)^jS^{-j}u\\
    &=(SS^{-1}TSS^{-1}-\lambda SIS^{-j})u\\
    &=(T-\lambda I)u.
\end{align*}
Hence $u=S^{j}v$ is a generalized eigenvector of $T$.

To complete the proof, suppose $v_1,\ldots,v_n$ are a basis of $G(\lambda,T)$. 
Via our work above, it follows that $S^{-j}v_1,\ldots,S^{-j}v_n$ is linearly independent and $S^{-j}v_1,\ldots,S^{-j}v_n\in G(\lambda,S^{-1}TS)$. 
To show $S^{-j}v_1,\ldots,S^{-j}v_n$ spans $G(\lambda,S^{-1}TS)$, suppose $u\in G(\lambda,S^{-1}TS)$. 
Via our work above, $S^ju\in G(\lambda,T)$ and there exist $a_1,\ldots a_n$ such that 
\[S^ju=a_1v_1+\cdots +a_nv_n.\]
Applying $S^{-j}$ to both sides, we have
\[u=a_1S^{-j}v_1+\cdots +a_nS^{-j}v_n\]
and it follows that $u\in\operatorname{span}(S^{-j}v_1,\ldots,S^{-j}v_n)$. 
Hence $S^{-j}v_1,\ldots,S^{-j}v_n$ is a basis of $G(\lambda,S^{-1}TS)$ and it follows that 
\[\operatorname{dim}G(\lambda,T)=n=\operatorname{dim}G(\lambda,S^{-1}TS).\]
Therefore, $T$ and $S^{-1}TS$ have the same eigenvalues with the same multiplicities.

\clearpage

\section{Distinct eigenvalues when $\operatorname{null}T^{n-2}\neq\operatorname{null}T^{n-1}$}
\subsection*{Problem statement}
Suppose $V$ is an $n$-dimensional complex vector space and $T$ is an operator on $V$ such that $\operatorname{null}T^{n-2}\neq\operatorname{null}T^{n-1}$. 
Prove that $T$ has at most two distinct eigenvalues.

\subsection*{Solution}
Let's first note that $T$ is not injective and $0$ is an eigenvalue. 
If $T$ were injective, then $\{0\}=\operatorname{null}T^0=\operatorname{null}T^1$, and via Theorem 8.3 (`Equality in the sequence of null spaces'), it would follow that $\operatorname{null}T^{n-2}=\operatorname{null}T^{n-1}$ which is a contradiction.

Let's now think about $\operatorname{dim}\operatorname{null}T^{n-1}$. 
Following from the proof of Theorem 8.4 (`Null spaces stop growing'), we have
\[\{0\}=\operatorname{null}T^0\subsetneq\operatorname{null}T^1\subsetneq\cdots\operatorname{null}T^{n-2}\subsetneq\operatorname{null}T^{n-1}\]
where at each of the strict inclusions in the chain above, the dimension increases by at least 1. Hence, we have
\[n-1\leq\operatorname{dim}\operatorname{null}T^{n-1}\leq\operatorname{dim}\operatorname{null}(T-0I)^{n}=\operatorname{dim}G(0,T).\]
Thus, the multiplicity of $0$ is either $n-1$ or $n$.

If the multiplicity of $0$ is $n-1$, then Theorem 8.26 (`Sum of the multiplicities equals $\operatorname{dim}V$') implies another distinct eigenvalue exists with a multiplicity of $1$ and $T$ has two distinct eigenvalues. 
If the multiplicity of $0$ is $n$, then Theorem 8.26 implies $0$ is the only eigenvalue and $T$ has one distinct eigenvalues. 
Therefore, $T$ has at most two distinct eigenvalues.

\clearpage

\section{Basis of eigenvectors iff they are generalized}
\subsection*{Problem statement}
Suppose $V$ is a complex vector space and $T\in\mathcal{L}(V)$. 
Prove that $V$ has a basis consisting of eigenvectors of $T$ if and only if every generalized eigenvector of $T$ is an eigenvector of $T$.

\subsection*{Solution}
\subsubsection*{First Direction}
Suppose $V$ has a basis consisting of eigenvectors of $T$. Via Theorem 5.41 (`Conditions equivalent to diagonalizability'), it follows that 
\begin{equation}\label{eq:eigenspace_direct_sum}
    V=E(\lambda_1,T)\oplus\cdots\oplus E(\lambda_m,T).
\end{equation}
The definition of generalized eigenspace (Definition 8.10) implies 
\newline
$E(\lambda_j,T)\subset G(\lambda_j,T)$, hence we have $\operatorname{dim}E(\lambda_j,T)\leq \operatorname{dim}G(\lambda_j,T)$. 

To show that the inequality is an equality, let's first note that from Theorem 8.21 (`Description of operators on complex vector spaces') we can write
\begin{equation}\label{eq:g_eigenspace_direct_sum}
    V=G(\lambda_1,T)\oplus\cdots\oplus G(\lambda_m,T).
\end{equation}
Suppose $\operatorname{dim}E(\lambda_j,T)< \operatorname{dim}G(\lambda_j,T)$. 
It follows that there exists $v\in V$ such that $v\notin E(\lambda_j,T)$ and $v\in G(\lambda_j,T)$. 
Our direct sum (\ref{eq:eigenspace_direct_sum}) implies that 
\[v=a_1v_1+\cdots+a_{j-1}v_{j-1}+0(v_j)+a_{j+1}v_{j+1}+\cdots+a_mv_m\]
where $v_k\in E(\lambda_k,T)$. Given $E(\lambda_j,T)\subset G(\lambda_j,T)$, that decomposition is also a valid decomposition in (\ref{eq:g_eigenspace_direct_sum}).
However, since $v\in G(\lambda_j,T)$, direct sum (\ref{eq:g_eigenspace_direct_sum}) implies that
\[v=0(v_1)+\cdots+0(v_{j-1})+a_jv_j+0(v_{j+1})+\cdots+0(v_m)\]
where $a_j\neq 0$ and $v_j\neq 0$\footnote{
Note that $v\notin E(\lambda_j,T)$ implies that $v\neq 0$.
}. 
Hence we have two different representations of $v$ and (\ref{eq:g_eigenspace_direct_sum}) is not a direct sum, leading to a contradiction. 

Thus, $\operatorname{dim}E(\lambda_j,T)=\operatorname{dim}G(\lambda_j,T)$, and since $E(\lambda_j,T)\subset G(\lambda_j,T)$, it follows that $E(\lambda_j,T)=G(\lambda_j,T)$. 
Therefore, every generalized eigenvector of $T$ is an eigenvector of $T$.

\subsubsection*{Second Direction}
Suppose every generalized eigenvector of $T$ is an eigenvector of $T$. 
Unlike the complexity of the \textbf{First Direction}, it clearly follows from Theorem 8.23 (`A basis of generalized eigenvectors') that $V$ has a basis consisting of eigenvectors of $T$.

\clearpage

\section{Find a square root of $I+N$}
\subsection*{Problem statement}
Define $N\in\mathcal{L}(\mathbf{F}^5)$ by
\[N(x_1,x_2,x_3,x_4,x_5)=(2x_2,3x_3,-x_4,4x_5,0).\]
Find a square root of $I+N$.

\subsection*{Solution}
Via Theorem 8.31 (`Identity plus nilpotent has a square root'), we know that there must exist a square root of $I+N$, but we are only provided partial guidance for how to find it. 
Theorem 8.31 tells us that the square root of $I+N$ will be of the form
\[I+a_1N+a_2N^2+a_3N^3+a_4N^4\]
via the Taylor series for the function $\sqrt{1+x}$ and the observation that $N^5=0$. 
To solve for $a_1,a_2,a_3,a_4$, we can write
\begin{multline*}
    (I+a_1N+a_2N^2+a_3N^3+a_4N^4)^2\\
    = I + 2a_1N + (a_1^2 + 2a_2)N^2\\
    + (2a_1a_2 + 2a_3)N^3 + (2a_1a_3 + 2a_4)N^4 +\ldots
\end{multline*}
where the other terms vanish since $N^5=0$. 
We are left to solve for the coefficients such that 
\begin{gather*} 
    2a_1=1,\\
    a_1^2 + 2a_2=0,\\
    2a_1a_2 + 2a_3=0,\\
    2a_1a_3 + 2a_4=0.
\end{gather*}
Via some simple calculations, it follows that $a_1=\frac{1}{2},a_2=-\frac{1}{8},a_3=\frac{1}{16},a_4=-\frac{1}{32}$ and we can write the square root of $I+N$ as
\[I+\frac{1}{2}N-\frac{1}{8}N^2+\frac{1}{16}N^3-\frac{1}{32}N^4.\]
Via another set of simple calculations, we can write
\begin{multline*}
    \sqrt{I+N}(x_1,x_2,x_3,x_4,x_5)=(x_1+x_2-\frac{3}{4}x_3-\frac{3}{8}x_4+\frac{3}{4}x_5,x_2+\frac{3}{2}x_3+\frac{3}{8}x_4-\frac{3}{4}x_5,\\
    x_3-\frac{1}{2}x_4+\frac{1}{2}x_5,x_4+2x_5,x_5).
\end{multline*}


\end{document}